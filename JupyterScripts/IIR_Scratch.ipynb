{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.pool import StaticPool\n",
    "from decimal import Decimal\n",
    "import datetime\n",
    "import os\n",
    "pd.set_option('display.max_columns', None)\n",
    "#DB Details\n",
    "connection_type = \"mssql+pyodbc\"\n",
    "dataportal_db_user = \"dataportal\"\n",
    "dataportal_db_password = \"FatShamingMarc\"\n",
    "dataportal_db_ip = \"10.8.4.35\"\n",
    "db_port = \"1433\"\n",
    "db_prod_name = \"dataportal_prod\"\n",
    "db_driver = \"ODBC+DRIVER+17+for+SQL+Server\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection_string(connection_type, user, password, host, port, database_name, driver):\n",
    "    return (\n",
    "        \"{connection_type}://{user}:{password}@{host}:{port}/{database_name}\"\n",
    "        \"?driver={driver}\").format(\n",
    "        connection_type=connection_type,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database_name=database_name,\n",
    "        driver=driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load for Offline Events file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'OfflineEvents20200825.csv'\n",
    "path = 'C:\\\\Temp\\\\IIR\\\\'\n",
    "filepath = path + filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapOfflineEventsDataframe(df, filename):\n",
    "    df.loc[df.PADD_REG == 'I', 'PADD_REG'] = \"PAD District 1\"\n",
    "    df.loc[df.PADD_REG == 'II', 'PADD_REG'] = \"PAD District 2\"\n",
    "    df.loc[df.PADD_REG == 'III', 'PADD_REG'] = \"PAD District 3\"\n",
    "    df.loc[df.PADD_REG == 'IV', 'PADD_REG'] = \"PAD District 4\"\n",
    "    df.loc[df.PADD_REG == 'V', 'PADD_REG'] = \"PAD District 5\"\n",
    "    df['ORIGIN_FILE'] = filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['COMMENTS'] = df['COMMENTS'].fillna('').str.replace(\"'\", \"''\")\n",
    "#df['AREA_NAME'] = df['AREA_NAME'].str.replace(\"'\", \"''\")\n",
    "#df['OWNER_NAME'] = df['OWNER_NAME'].str.replace(\"'\", \"''\")\n",
    "#df['PLANT_NAME'] = df['OWNER_NAME'].str.replace(\"'\", \"''\")\n",
    "# def insertOfflineEvents_CsvToSql_Fast(engine, path, filename, columns):\n",
    "#     filepath = path + filename\n",
    "#     df = pd.read_csv(filepath, low_memory=False)[columns]\n",
    "#     mapOfflineEventsDataframe(df, filename)\n",
    "#     queryText = ''\n",
    "#     for index, row in df.iterrows():\n",
    "#         queryText += \\\n",
    "# f\"\"\"INSERT INTO dataportal_prod..data_iir_offlineevents values ('{row.RELEASE_DT}',{int(row.EVENT_ID)},'{row.UNIT_NAME}',{int(row.UNIT_ID)},{int(row.AREA_ID)},'{row.AREA_NAME}','{row.OWNER_NAME}',\\\n",
    "# {int(row.PLANT_ID)},'{row.PLANT_NAME}','{row.U_STATUS}','{row.UTYPE_DESC}',{\"NULL\" if pd.isna(row.U_CAPACITY) else int(row.U_CAPACITY)},\\\n",
    "# {\"NULL\" if pd.isna(row.CAP_OFFLINE) else int(row.CAP_OFFLINE)},'{row.START_DATE}','{row.END_DATE}','{row.PRECISION}','{row.EVENT_TYPE}',\\\n",
    "# '{row.E_STATUS}','{\"NULL\" if pd.isna(row.E_CAUSE) else row.E_CAUSE}',{int(row.E_DURATION)},'{row.PADD_REG}','{\"NULL\" if pd.isna(row.COMMENTS) else row.COMMENTS}','{row.ORIGIN_FILE}');\\n\"\"\"\n",
    "\n",
    "#         if index % 250 == 0 or index == df.shape[0] - 1:\n",
    "#             #print(index)\n",
    "#             with engine.begin() as connection:\n",
    "#                 connection.execute(queryText)\n",
    "#                 queryText=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define columns we're interested in from the file\n",
    "columns=['RELEASE_DT', 'EVENT_ID', 'UNIT_NAME', 'UNIT_ID', 'AREA_ID', 'AREA_NAME', 'OWNER_NAME', 'PLANT_ID', 'PLANT_NAME', \n",
    "         'U_STATUS', 'UTYPE_DESC', 'U_CAPACITY', 'CAP_OFFLINE', 'START_DATE', 'END_DATE', 'PRECISION', 'EVENT_TYPE', \n",
    "         'E_STATUS', 'E_CAUSE', 'E_DURATION', 'PADD_REG', 'COMMENTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_columns={\"RELEASE_DT\": \"PUBLISH_DATE\", \"U_STATUS\":\"UNIT_STATUS\", \"U_CAPACITY\":\"CAPACITY\", \"CAP_OFFLINE\":\"CAPACITY_OFFLINE\", \"START_DATE\": \"EVENT_START\", \"END_DATE\": \"EVENT_END\", \n",
    "                 \"PRECISION\": \"DATE_PRECISION\", \"E_STATUS\": \"EVENT_STATUS\", \"E_CAUSE\": \"EVENT_CAUSE\", \"E_DURATION\" : \"EVENT_DURATION\", \"PADD_REG\": \"PAD_DISTRICT\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = create_connection_string(connection_type,dataportal_db_user,dataportal_db_password,dataportal_db_ip,db_port,db_prod_name, db_driver)\n",
    "engine = create_engine(connection_string, poolclass=StaticPool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual Run\n",
    "filepath = path + filename\n",
    "df = pd.read_csv(filepath, low_memory=False)[columns]\n",
    "mapOfflineEventsDataframe(df, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns = renamed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert to SQL\n",
    "df.to_sql('data_iir_offlineevents', connection_string, if_exists='append', index=False,\n",
    "        dtype={ 'PUBLISH_DATE' :   sa.DateTime(),\n",
    "                'EVENT_ID' :       sa.types.INTEGER(),\n",
    "                'UNIT_NAME' :     sa.types.VARCHAR(100),\n",
    "                'UNIT_ID' :       sa.types.INTEGER(),\n",
    "                'AREA_ID' :       sa.types.INTEGER(),\n",
    "                'AREA_NAME' :     sa.types.VARCHAR(100),\n",
    "                'OWNER_NAME' :     sa.types.VARCHAR(100),\n",
    "                'PLANT_ID' :       sa.types.INTEGER(),\n",
    "                'PLANT_NAME' :     sa.types.VARCHAR(100),\n",
    "                'UNIT_STATUS' : sa.types.VARCHAR(30),\n",
    "                'UTYPE_DESC' : sa.types.VARCHAR(100),\n",
    "                'CAPACITY' : sa.types.INTEGER(),\n",
    "                'CAPACITY_OFFLINE' : sa.types.INTEGER(),\n",
    "                'EVENT_START' :  sa.DateTime(),\n",
    "                'EVENT_END' :  sa.DateTime(),\n",
    "                'DATE_PRECISION' : sa.types.VARCHAR(16) ,\n",
    "                'EVENT_TYPE' : sa.types.VARCHAR(30),         \n",
    "                'EVENT_STATUS' : sa.types.VARCHAR(30),\n",
    "                'EVENT_CAUSE' : sa.types.VARCHAR(65),\n",
    "                'EVENT_DURATION' : sa.types.INTEGER(),\n",
    "                'PAD_DISTRICT' : sa.types.VARCHAR(65),\n",
    "                'COMMENTS' : sa.types.TEXT ,\n",
    "                'ORIGIN_FILE' : sa.types.VARCHAR(65)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load for Plant File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'PLANT_20200801.csv'\n",
    "path = 'C:\\\\Temp\\\\IIR\\\\'\n",
    "filepath = path + filename\n",
    "df = pd.read_csv(filepath, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(0,'PUBLISH_DATE', datetime.date(2020,8, 1))\n",
    "df['ORIGIN_FILE'] = filename\n",
    "df = df.rename(columns={\"STARTUP\":\"STARTUP_DATE\", \"SHUTDOWN\":\"SHUTDOWN_DATE\", \"UNION\":\"UNION_FACILITY\", \"COUNT\":\"COUNT_NUMBER\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = create_connection_string(connection_type,dataportal_db_user,dataportal_db_password,dataportal_db_ip,db_port,db_prod_name, db_driver)\n",
    "engine = create_engine(connection_string, poolclass=StaticPool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_sql('data_iir_plant_reference', connection_string, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #For converting column to nullable integers use (capital Int):\n",
    "# # All int columns\n",
    "# df[\"PARENT_ID\"] = df[\"PARENT_ID\"].astype('Int64')\n",
    "# df[\"OWNER_ID2\"] = df[\"OWNER_ID2\"].astype('Int64')\n",
    "# df[\"PLT_MGR_ID\"] = df[\"PLT_MGR_ID\"].astype('Int64')\n",
    "# df[\"MAINT_ID\"] = df[\"MAINT_ID\"].astype('Int64')\n",
    "# df[\"BUYER_ID\"] = df[\"BUYER_ID\"].astype('Int64')\n",
    "# df[\"ENGR_ID\"] = df[\"ENGR_ID\"].astype('Int64')\n",
    "# df[\"ENVIR_ID\"] = df[\"ENVIR_ID\"].astype('Int64')\n",
    "# df[\"SAFETY_ID\"] = df[\"SAFETY_ID\"].astype('Int64')\n",
    "# df[\"UTILITY_ID\"] = df[\"UTILITY_ID\"].astype('Int64')\n",
    "# df[\"IT_ID\"] = df[\"IT_ID\"].astype('Int64')\n",
    "# df[\"HR_ID\"] = df[\"HR_ID\"].astype('Int64')\n",
    "# df[\"NO_EMP\"] = df[\"NO_EMP\"].astype('Int64')\n",
    "# df[\"SHIFTS_DAY\"] = df[\"SHIFTS_DAY\"].astype('Int64')\n",
    "# df[\"MNT_STAFF\"] = df[\"MNT_STAFF\"].astype('Int64')\n",
    "# df[\"CAPACITY\"] = df[\"CAPACITY\"].astype('Int64')\n",
    "# df[\"ELECT_CONN\"] = df[\"ELECT_CONN\"].astype('Int64')\n",
    "# df[\"PRIM_BUS\"] = df[\"PRIM_BUS\"].astype('Int64')\n",
    "# df[\"ALL_BUS\"] = df[\"ALL_BUS\"].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', 182)\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[pd.isna(df[\"PARENT_ID\"])]\n",
    "#df[df[\"PLANT_ID\"] == 2016126]\n",
    "#df[df[\"PLANT_ID\"] == 2016126][\"BUYCONT_OS\"]\n",
    "#df.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force convert to dates\n",
    "df[\"PLCONT_QC\"] = pd.to_datetime(df[\"PLCONT_QC\"], format='%Y%m%d')\n",
    "df[\"MNTCONT_QC\"] = pd.to_datetime(df[\"MNTCONT_QC\"], format='%Y%m%d')\n",
    "df[\"BUYCONT_QC\"] = pd.to_datetime(df[\"BUYCONT_QC\"], format='%Y%m%d')\n",
    "df[\"ENGCONT_QC\"] = pd.to_datetime(df[\"ENGCONT_QC\"], format='%Y%m%d')\n",
    "df[\"ENVCONT_QC\"] = pd.to_datetime(df[\"ENVCONT_QC\"], format='%Y%m%d')\n",
    "df[\"SAFCONT_QC\"] = pd.to_datetime(df[\"SAFCONT_QC\"], format='%Y%m%d')\n",
    "df[\"UTCONT_QC\"] = pd.to_datetime(df[\"UTCONT_QC\"], format='%Y%m%d')\n",
    "df[\"ITCONT_QC\"] = pd.to_datetime(df[\"ITCONT_QC\"], format='%Y%m%d')\n",
    "df[\"HRCONT_QC\"] = pd.to_datetime(df[\"HRCONT_QC\"], format='%Y%m%d')\n",
    "df[\"STARTUP_DATE\"] = pd.to_datetime(df[\"STARTUP_DATE\"], format='%Y%m%d')\n",
    "df[\"SHUTDOWN_DATE\"] = pd.to_datetime(df[\"SHUTDOWN_DATE\"], format='%Y%m%d')\n",
    "df[\"KICKOFF\"] = pd.to_datetime(df[\"KICKOFF\"], format='%Y%m%d')\n",
    "df[\"QC_DATE\"] = pd.to_datetime(df[\"QC_DATE\"], format='%Y%m%d')\n",
    "df[\"UP_DATE\"] = pd.to_datetime(df[\"UP_DATE\"], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNION column is defined as boolean, not sure why value 2 is in there. Force change to 1\n",
    "#df.loc[df[\"UNION_FACILITY\"] == 2, \"UNION_FACILITY\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check example\n",
    "#df[df[\"PLANT_ID\"] == 2016126]\n",
    "#df[pd.isna(df[\"PARENT_ID\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df[\"PLANT_ID\"] == 2016126]\n",
    "#for index, row in df.iterrows():\n",
    "#    currentdf = df[df[\"PLANT_ID\"] == row.PLANT_ID]\n",
    "df.to_sql('data_iir_plant_reference', connection_string, if_exists='append', index=False,\n",
    "        dtype={'PUBLISH_DATE' :   sa.DateTime(),\n",
    "'PLANT_ID' :       sa.types.Integer,\n",
    "'PARENT_ID' :      sa.types.INTEGER(),\n",
    "'PARENTNAME' :     sa.types.VARCHAR(100),\n",
    "'OWNER_ID1' :      sa.types.INTEGER(),\n",
    "'OWNER_NAME' :     sa.types.VARCHAR(100),\n",
    "'OWNER_ID2' :      sa.types.INTEGER(),\n",
    "'OWN_NAME2' :      sa.types.VARCHAR(100),\n",
    "'OPER_ID' :               sa.types.INTEGER(),\n",
    "'OPER_NAME' :      sa.types.VARCHAR(100),\n",
    "'PLANT_NAME' :     sa.types.VARCHAR(100),\n",
    "'PHYS_ADDR' :      sa.types.VARCHAR(255),\n",
    "'PHYS_ADDR2' :     sa.types.VARCHAR(255),\n",
    "'PHYS_CITY' :      sa.types.VARCHAR(95),\n",
    "'PHYS_STATE' :     sa.types.VARCHAR(20),\n",
    "'P_ST_NAME' :      sa.types.VARCHAR(95),\n",
    "'PHYS_ZIP' :       sa.types.VARCHAR(20),\n",
    "'PHYS_CSZ' :       sa.types.VARCHAR(95),\n",
    "'P_COUNTRY' :      sa.types.VARCHAR(95),\n",
    "'MAIL_ADDR' :      sa.types.VARCHAR(255),\n",
    "'MAIL_ADDR2' :     sa.types.VARCHAR(255),\n",
    "'MAIL_CITY' :      sa.types.VARCHAR(95),\n",
    "'MAIL_STATE' :     sa.types.VARCHAR(20),\n",
    "'M_ST_NAME' :      sa.types.VARCHAR(100),\n",
    "'MAIL_ZIP' :       sa.types.VARCHAR(95),\n",
    "'MAIL_CSZ' :       sa.types.VARCHAR(95),\n",
    "'M_COUNTRY' :      sa.types.VARCHAR(95),\n",
    "'COUNTY_ID' :      sa.types.VARCHAR(20),\n",
    "'COUNTYNAME' :     sa.types.VARCHAR(95),\n",
    "'PEC_ZONE' :       sa.types.VARCHAR(95),\n",
    "'COMPLEX_RG' :     sa.types.VARCHAR(95),\n",
    "'MARKET_REG' :     sa.types.VARCHAR(95),\n",
    "'NERC_REGIO' :     sa.types.VARCHAR(95),\n",
    "'NERC_SRGN' :      sa.types.VARCHAR(95),\n",
    "'CTRLAREAID' :     sa.types.VARCHAR(95),\n",
    "'CTLAREADES' :     sa.types.VARCHAR(95),\n",
    "'PAD_DIST' :       sa.types.VARCHAR(95),\n",
    "'PHONE' :          sa.types.VARCHAR(95),\n",
    "'FAX' :            sa.types.VARCHAR(95),\n",
    "'IND_CODE' :       sa.types.VARCHAR(20),\n",
    "'IND_DESC' :       sa.types.VARCHAR(95),\n",
    "'SIC_CODE' :       sa.types.INTEGER(),\n",
    "'SIC_DESC' :       sa.types.VARCHAR(95),\n",
    "'PL_STATUS' :      sa.types.VARCHAR(95),\n",
    "'PLT_MGR_ID' :     sa.types.INTEGER(),\n",
    "'PLANT_TLE' :      sa.types.VARCHAR(95),\n",
    "'PLANT_FNME' :     sa.types.VARCHAR(95),\n",
    "'PLANT_MNME' :     sa.types.VARCHAR(95),\n",
    "'PLANT_LNME' :     sa.types.VARCHAR(95),\n",
    "'PLANT_JRSR' :     sa.types.VARCHAR(95),\n",
    "'PLCONT_PH' :      sa.types.VARCHAR(95),\n",
    "'PLCONT_EX' :      sa.types.VARCHAR(95),\n",
    "'PLANTEMAIL' :     sa.types.VARCHAR(95),\n",
    "'PLCONT_OS' :      sa.types.Boolean,\n",
    "'PLCONT_QC' :      sa.DateTime(),\n",
    "'MAINT_ID' :       sa.types.INTEGER(),\n",
    "'MAINT_TLE' :      sa.types.VARCHAR(95),\n",
    "'MAINT_FNME' :     sa.types.VARCHAR(95),\n",
    "'MAINT_MNME' :     sa.types.VARCHAR(95),\n",
    "'MAINT_LNME' :     sa.types.VARCHAR(95),\n",
    "'MAINT_JRSR' :     sa.types.VARCHAR(95),\n",
    "'MNTCONT_PH' :     sa.types.VARCHAR(95),\n",
    "'MNTCONT_EX' :     sa.types.VARCHAR(95),\n",
    "'MAINTEMAIL' :     sa.types.VARCHAR(95),\n",
    "'MNTCONT_OS' :     sa.types.Boolean,\n",
    "'MNTCONT_QC' :     sa.DateTime(),\n",
    "'BUYER_ID' :       sa.types.INTEGER(),\n",
    "'BUYER_TLE' :      sa.types.VARCHAR(95),\n",
    "'BUYER_FNME' :     sa.types.VARCHAR(95),\n",
    "'BUYER_MNME' :     sa.types.VARCHAR(95),\n",
    "'BUYER_LNME' :     sa.types.VARCHAR(95),\n",
    "'BUYER_JRSR' :     sa.types.VARCHAR(95),\n",
    "'BUYCONT_PH' :     sa.types.VARCHAR(95),\n",
    "'BUYCONT_EX' :     sa.types.VARCHAR(95),\n",
    "'BUYEREMAIL' :     sa.types.VARCHAR(95),\n",
    "'BUYCONT_OS' :     sa.types.Boolean,\n",
    "'BUYCONT_QC' :     sa.DateTime(),\n",
    "'ENGR_ID' :               sa.types.INTEGER(),\n",
    "'ENGR_TLE' :       sa.types.VARCHAR(95),\n",
    "'ENGR_FNME' :      sa.types.VARCHAR(95),\n",
    "'ENGR_MNME' :      sa.types.VARCHAR(95),\n",
    "'ENGR_LNME' :      sa.types.VARCHAR(95),\n",
    "'ENGR_JRSR' :      sa.types.VARCHAR(95),\n",
    "'ENGCONT_PH' :     sa.types.VARCHAR(95),\n",
    "'ENGCONT_EX' :     sa.types.VARCHAR(95),\n",
    "'ENGR_EMAIL' :     sa.types.VARCHAR(95),\n",
    "'ENGCONT_OS' :     sa.types.Boolean,\n",
    "'ENGCONT_QC' :     sa.DateTime(),\n",
    "'ENVIR_ID' :       sa.types.INTEGER(),\n",
    "'ENVIR_TLE' :      sa.types.VARCHAR(95),\n",
    "'ENVIR_FNME' :     sa.types.VARCHAR(95),\n",
    "'ENVIR_MNME' :     sa.types.VARCHAR(95),\n",
    "'ENVIR_LNME' :     sa.types.VARCHAR(95),\n",
    "'ENVIR_JRSR' :     sa.types.VARCHAR(95),\n",
    "'ENVCONT_PH' :     sa.types.VARCHAR(95),\n",
    "'ENVCONT_EX' :     sa.types.VARCHAR(95),\n",
    "'ENVIREMAIL' :     sa.types.VARCHAR(95),\n",
    "'ENVCONT_OS' :     sa.types.Boolean,\n",
    "'ENVCONT_QC' :     sa.DateTime(),\n",
    "'SAFETY_ID' :      sa.types.INTEGER(),\n",
    "'SAFE_TITLE' :     sa.types.VARCHAR(95),\n",
    "'SAFE_FNME' :      sa.types.VARCHAR(95),\n",
    "'SAFE_MNME' :      sa.types.VARCHAR(95),\n",
    "'SAFE_LNME' :      sa.types.VARCHAR(95),\n",
    "'SAFE_JRSR' :      sa.types.VARCHAR(95),\n",
    "'SAFCONT_PH' :     sa.types.VARCHAR(95),\n",
    "'SAFCONT_EX' :     sa.types.VARCHAR(95),\n",
    "'SAFE_EMAIL' :     sa.types.VARCHAR(95),\n",
    "'SAFCONT_OS' :     sa.types.Boolean,\n",
    "'SAFCONT_QC' :     sa.DateTime(),\n",
    "'UTILITY_ID' :     sa.types.INTEGER(),\n",
    "'UTIL_TLE' :       sa.types.VARCHAR(95),\n",
    "'UTIL_FNME' :      sa.types.VARCHAR(95),\n",
    "'UTIL_MNME' :      sa.types.VARCHAR(95),\n",
    "'UTIL_LNME' :      sa.types.VARCHAR(95),\n",
    "'UTIL_JRSR' :      sa.types.VARCHAR(95),\n",
    "'UTCONT_PH' :      sa.types.VARCHAR(95),\n",
    "'UTCONT_EX' :      sa.types.VARCHAR(95),\n",
    "'UTIL_EMAIL' :     sa.types.VARCHAR(95),\n",
    "'UTCONT_OS' :      sa.types.Boolean,\n",
    "'UTCONT_QC' :      sa.DateTime(),\n",
    "'IT_ID' :   sa.types.INTEGER(),\n",
    "'IT_TITLE' :       sa.types.VARCHAR(95),\n",
    "'IT_FNME' :        sa.types.VARCHAR(95),\n",
    "'IT_MNME' :        sa.types.VARCHAR(95),\n",
    "'IT_LNME' :        sa.types.VARCHAR(95),\n",
    "'IT_JRSR' :        sa.types.VARCHAR(95),\n",
    "'ITCONT_PH' :      sa.types.VARCHAR(95),\n",
    "'ITCONT_EX' :      sa.types.VARCHAR(95),\n",
    "'IT_EMAIL' :       sa.types.VARCHAR(95),\n",
    "'ITCONT_OS' :      sa.types.Boolean,\n",
    "'ITCONT_QC' :      sa.DateTime(),\n",
    "'HR_ID' :          sa.types.INTEGER(),\n",
    "'HR_TITLE' :       sa.types.VARCHAR(95),\n",
    "'HR_FNME' :        sa.types.VARCHAR(95),\n",
    "'HR_MNME' :        sa.types.VARCHAR(95),\n",
    "'HR_LNME' :        sa.types.VARCHAR(95),\n",
    "'HR_JRSR' :        sa.types.VARCHAR(95),\n",
    "'HRCONT_PH' :      sa.types.VARCHAR(95),\n",
    "'HRCONT_EX' :      sa.types.VARCHAR(95),\n",
    "'HR_EMAIL' :       sa.types.VARCHAR(95),\n",
    "'HRCONT_OS' :      sa.types.Boolean,\n",
    "'HRCONT_QC' :      sa.DateTime(),\n",
    "'NO_EMP' :  sa.types.INTEGER(),\n",
    "'SHIFTS_DAY' :     sa.types.INTEGER(),\n",
    "'MNT_STAFF' :      sa.types.INTEGER(),\n",
    "'MNT_CNTRCT' :     sa.types.VARCHAR(20),\n",
    "'STARTUP_DATE' :   sa.DateTime(),  \n",
    "'SU_PRECISN' :     sa.types.VARCHAR(16),\n",
    "'SHUTDOWN_DATE' :  sa.DateTime(),  \n",
    "'SD_PRECISN' :     sa.types.VARCHAR(16),\n",
    "'KICKOFF' :        sa.DateTime(),\n",
    "'KO_PRECISN' :     sa.types.VARCHAR(16),\n",
    "'UNION_FACILITY' :        sa.types.INTEGER(),   \n",
    "'CAPACITY' :       sa.types.INTEGER(),\n",
    "'CAPACITY_U' :     sa.types.VARCHAR(20), \n",
    "'WASTEWATER' :     sa.types.Boolean,\n",
    "'FUEL_TYPE1' :     sa.types.VARCHAR(95),\n",
    "'FUEL_TYPE2' :     sa.types.VARCHAR(95),\n",
    "'ELECT_GEN' :      sa.types.Boolean,\n",
    "'STEAM_PROD' :     sa.types.Boolean,\n",
    "'ELECT_CONN' :     sa.types.INTEGER(),\n",
    "'ELECONDESC' :     sa.types.VARCHAR(95),\n",
    "'ELECCONSUM' :     sa.types.VARCHAR(20),\n",
    "'PRIM_BUS' :       sa.types.INTEGER(),\n",
    "'ALL_BUS' :        sa.types.INTEGER(),\n",
    "'NG_CONN' :        sa.types.INTEGER(),\n",
    "'NGCONDESC' :      sa.types.VARCHAR(95),\n",
    "'NGCONSUM' :       sa.types.VARCHAR(20),\n",
    "'TR_HWY' :         sa.types.Boolean,\n",
    "'TR_RAIL' :        sa.types.Boolean,\n",
    "'TR_DOCK' :        sa.types.Boolean,\n",
    "'TR_PIPE' :        sa.types.Boolean,\n",
    "'TR_SPUR' :        sa.types.Boolean,\n",
    "'TR_PORT' :        sa.types.Boolean,\n",
    "'TR_TANK' :        sa.types.Boolean,\n",
    "'QC_DATE' :        sa.DateTime(),\n",
    "'UP_DATE' :        sa.DateTime(),\n",
    "'COUNT_NUMBER' :          sa.types.INTEGER(),\n",
    "'LATITUDE' :       sa.Float(),\n",
    "'LONGITUDE' :      sa.Float(),\n",
    "'ORIGIN_FILE' :    sa.types.VARCHAR(95)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load for UNIT File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'UNIT_20200801.csv'\n",
    "path = 'C:\\\\Temp\\\\IIR\\\\'\n",
    "filepath = path + filename\n",
    "df = pd.read_csv(filepath, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_columns={\"U_STATUS\":\"UNIT_STATUS\", \"STARTUP\":\"STARTUP_DATE\", \"U_CAPACITY\":\"CAPACITY\", \"COUNT\" : \"COUNT_NUMBER\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(0,'PUBLISH_DATE', datetime.date(2020,8, 1))\n",
    "df = df.rename(columns=renamed_columns)\n",
    "df['ORIGIN_FILE'] = filename\n",
    "df[\"STARTUP_DATE\"] = pd.to_datetime(df[\"STARTUP_DATE\"], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for index, row in df.iterrows():\n",
    "#    currentdf = df[df[\"UNIT_ID\"] == row.UNIT_ID]\n",
    "df.to_sql('data_iir_unit_reference', connection_string, if_exists='append', index=False,\n",
    "    dtype={ 'PUBLISH_DATE' :   sa.DateTime(),\n",
    "            'UNIT_ID' : sa.types.INTEGER(),\n",
    "            'UNIT_NAME' : sa.types.VARCHAR(100),\n",
    "            'AREA_ID' : sa.types.INTEGER(),\n",
    "            'AREA_NAME' : sa.types.VARCHAR(100),\n",
    "            'PLANT_ID' :sa.types.INTEGER(),\n",
    "            'PLANT_NAME' : sa.types.VARCHAR(100),\n",
    "            'OWNER_ID' : sa.types.INTEGER(),\n",
    "            'OWNER_NAME' : sa.types.VARCHAR(100),\n",
    "            'COUNTY_ID' : sa.types.VARCHAR(8),\n",
    "            'UNIT_STATE' : sa.types.VARCHAR(6),\n",
    "            'UNIT_STATUS' : sa.types.VARCHAR(20),  \n",
    "            'STARTUP_DATE' : sa.DateTime(), \n",
    "            'UNIT_TYPE' : sa.types.VARCHAR(50),\n",
    "            'CAPACITY' : sa.types.INTEGER(),\n",
    "            'CAP_UOM' : sa.types.VARCHAR(12),\n",
    "            'SIC_CODE' : sa.types.VARCHAR(4),\n",
    "            'PAD_DIST' : sa.types.INTEGER(),\n",
    "            'COUNT_NUMBER' : sa.types.INTEGER(),\n",
    "            'ORIGIN_FILE' : sa.types.VARCHAR(65)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load for TAROUND file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define columns we're interested in from the file\n",
    "columns=['DELV_DATE', 'OUTAGE_ID', 'UNIT_NAME', 'UNIT_ID', 'AREA_ID', 'AREA_NAME', 'OWNER_NAME', 'PLANT_ID', 'PLANT_NAME', \n",
    "         'U_STATUS', 'UTYPE_DESC', 'CHARGERATE', 'CAPACITY', 'TA_START', 'TA_END', 'PRECISION', 'OUTAGE_TYP', \n",
    "         'OUTAGE_STA', 'OUT_CAUSE', 'PERIOD', 'PAD_DIST', 'COMMENTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_columns={\"DELV_DATE\": \"PUBLISH_DATE\", \"OUTAGE_ID\": \"EVENT_ID\", \"U_STATUS\":\"UNIT_STATUS\", \n",
    "                 \"CHARGERATE\":\"CAPACITY\", \"CAPACITY\":\"CAPACITY_OFFLINE\", \n",
    "                 \"TA_START\": \"EVENT_START\", \"TA_END\": \"EVENT_END\", \"PRECISION\": \"DATE_PRECISION\", \"OUTAGE_TYP\" : \"EVENT_TYPE\",\n",
    "                 \"OUTAGE_STA\": \"EVENT_STATUS\", \"OUT_CAUSE\": \"EVENT_CAUSE\", \"PERIOD\" : \"EVENT_DURATION\", \"PAD_DIST\": \"PAD_DISTRICT\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'TAROUND_202008250103.csv'\n",
    "path = 'C:\\\\Temp\\\\IIR\\\\'\n",
    "filepath = path + filename\n",
    "df = pd.read_csv(filepath, low_memory=False)[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns=renamed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PUBLISH_DATE\"] = pd.to_datetime(df[\"PUBLISH_DATE\"], format='%Y%m%d')\n",
    "df[\"EVENT_START\"] = pd.to_datetime(df[\"EVENT_START\"], format='%Y%m%d')\n",
    "df[\"EVENT_END\"] = pd.to_datetime(df[\"EVENT_END\"], format='%Y%m%d')\n",
    "df['ORIGIN_FILE'] = filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert to SQL\n",
    "df.to_sql('data_iir_offlineevents', connection_string, if_exists='append', index=False,\n",
    "        dtype={ 'PUBLISH_DATE' :   sa.DateTime(),\n",
    "                'EVENT_ID' :       sa.types.INTEGER(),\n",
    "                'UNIT_NAME' :     sa.types.VARCHAR(100),\n",
    "                'UNIT_ID' :       sa.types.INTEGER(),\n",
    "                'AREA_ID' :       sa.types.INTEGER(),\n",
    "                'AREA_NAME' :     sa.types.VARCHAR(100),\n",
    "                'OWNER_NAME' :     sa.types.VARCHAR(100),\n",
    "                'PLANT_ID' :       sa.types.INTEGER(),\n",
    "                'PLANT_NAME' :     sa.types.VARCHAR(100),\n",
    "                'UNIT_STATUS' : sa.types.VARCHAR(30),\n",
    "                'UTYPE_DESC' : sa.types.VARCHAR(100),\n",
    "                'CAPACITY' : sa.types.INTEGER(),\n",
    "                'CAPACITY_OFFLINE' : sa.types.INTEGER(),\n",
    "                'EVENT_START' :  sa.DateTime(),\n",
    "                'EVENT_END' :  sa.DateTime(),\n",
    "                'DATE_PRECISION' : sa.types.VARCHAR(16) ,\n",
    "                'EVENT_TYPE' : sa.types.VARCHAR(30),         \n",
    "                'EVENT_STATUS' : sa.types.VARCHAR(30),\n",
    "                'EVENT_CAUSE' : sa.types.VARCHAR(65),\n",
    "                'EVENT_DURATION' : sa.types.INTEGER(),\n",
    "                'PAD_DISTRICT' : sa.types.VARCHAR(65),\n",
    "                'COMMENTS' : sa.types.VARCHAR(2000) ,\n",
    "                'ORIGIN_FILE' : sa.types.VARCHAR(65)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertTaroundFile(filename, path):\n",
    "    filepath = path + filename\n",
    "    #Define columns we're interested in from the file\n",
    "    columns=['DELV_DATE', 'OUTAGE_ID', 'UNIT_NAME', 'UNIT_ID', 'AREA_ID', 'AREA_NAME', 'OWNER_NAME', 'PLANT_ID', 'PLANT_NAME', \n",
    "             'U_STATUS', 'UTYPE_DESC', 'CHARGERATE', 'CAPACITY', 'TA_START', 'TA_END', 'PRECISION', 'OUTAGE_TYP', \n",
    "             'OUTAGE_STA', 'OUT_CAUSE', 'PERIOD', 'PAD_DIST', 'COMMENTS']\n",
    "    renamed_columns={\"DELV_DATE\": \"PUBLISH_DATE\", \"OUTAGE_ID\": \"EVENT_ID\", \"U_STATUS\":\"UNIT_STATUS\", \n",
    "                 \"CHARGERATE\":\"CAPACITY\", \"CAPACITY\":\"CAPACITY_OFFLINE\", \n",
    "                 \"TA_START\": \"EVENT_START\", \"TA_END\": \"EVENT_END\", \"PRECISION\": \"DATE_PRECISION\", \"OUTAGE_TYP\" : \"EVENT_TYPE\",\n",
    "                 \"OUTAGE_STA\": \"EVENT_STATUS\", \"OUT_CAUSE\": \"EVENT_CAUSE\", \"PERIOD\" : \"EVENT_DURATION\", \"PAD_DIST\": \"PAD_DISTRICT\"}\n",
    "    df = pd.read_csv(filepath, low_memory=False)[columns]\n",
    "    df = df.rename(columns=renamed_columns)\n",
    "    df[\"PUBLISH_DATE\"] = pd.to_datetime(df[\"PUBLISH_DATE\"], format='%Y%m%d')\n",
    "    df[\"EVENT_START\"] = pd.to_datetime(df[\"EVENT_START\"], format='%Y%m%d')\n",
    "    df[\"EVENT_END\"] = pd.to_datetime(df[\"EVENT_END\"], format='%Y%m%d')\n",
    "    df['ORIGIN_FILE'] = filename\n",
    "    df.to_sql('data_iir_offlineevents', connection_string, if_exists='append', index=False,\n",
    "        dtype={ 'PUBLISH_DATE' :   sa.DateTime(),\n",
    "                'EVENT_ID' :       sa.types.INTEGER(),\n",
    "                'UNIT_NAME' :     sa.types.VARCHAR(100),\n",
    "                'UNIT_ID' :       sa.types.INTEGER(),\n",
    "                'AREA_ID' :       sa.types.INTEGER(),\n",
    "                'AREA_NAME' :     sa.types.VARCHAR(100),\n",
    "                'OWNER_NAME' :     sa.types.VARCHAR(100),\n",
    "                'PLANT_ID' :       sa.types.INTEGER(),\n",
    "                'PLANT_NAME' :     sa.types.VARCHAR(100),\n",
    "                'UNIT_STATUS' : sa.types.VARCHAR(30),\n",
    "                'UTYPE_DESC' : sa.types.VARCHAR(100),\n",
    "                'CAPACITY' : sa.types.INTEGER(),\n",
    "                'CAPACITY_OFFLINE' : sa.types.INTEGER(),\n",
    "                'EVENT_START' :  sa.DateTime(),\n",
    "                'EVENT_END' :  sa.DateTime(),\n",
    "                'DATE_PRECISION' : sa.types.VARCHAR(16) ,\n",
    "                'EVENT_TYPE' : sa.types.VARCHAR(30),         \n",
    "                'EVENT_STATUS' : sa.types.VARCHAR(30),\n",
    "                'EVENT_CAUSE' : sa.types.VARCHAR(65),\n",
    "                'EVENT_DURATION' : sa.types.INTEGER(),\n",
    "                'PAD_DISTRICT' : sa.types.VARCHAR(65),\n",
    "                'COMMENTS' : sa.types.VARCHAR(2000) ,\n",
    "                'ORIGIN_FILE' : sa.types.VARCHAR(65)})\n",
    "    print(\"File %s uploaded successfully to the database\" % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing multiple files from local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Temp\\\\IIR\\\\TAROUND'\n",
    "extension='CSV'\n",
    "filename_pattern='TAROUND'\n",
    "file_names = [fn.upper() for fn in os.listdir(path)\n",
    "              if filename_pattern in fn.upper() and fn.upper().endswith(extension)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"select distinct(ORIGIN_FILE) from data_iir_offlineevents where origin_file like 'TAROUND%'\"\n",
    "current_db_files = [file.upper() for file in pd.read_sql(sql_query, connection_string)[\"ORIGIN_FILE\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in file_names:\n",
    "    #if name not in current_db_files:\n",
    "        insertTaroundFile(name, path)\n",
    "        print('Inserted' + name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing multiple files from ftp server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'250 Directory successfully changed.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ftplib import FTP\n",
    "global ftp\n",
    "\n",
    "ftp_site='ftp.industrialinfo.com'\n",
    "user='andurand'\n",
    "pw='361984'\n",
    "path = 'C:\\\\Temp\\\\IIR\\\\TAROUND\\\\'\n",
    "ftp_path='/data/'\n",
    "extension='CSV'\n",
    "filename_pattern='TAROUND'\n",
    "\n",
    "ftp = FTP(ftp_site, user=user, passwd=pw)\n",
    "ftp.cwd(ftp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve list of current inserted files from the database\n",
    "sql_query = \"select distinct(ORIGIN_FILE) from data_iir_offlineevents where origin_file like 'TAROUND%'\"\n",
    "connection_string = create_connection_string(connection_type,dataportal_db_user,dataportal_db_password,dataportal_db_ip,db_port,db_prod_name, db_driver)\n",
    "current_files = [file.upper() for file in pd.read_sql(sql_query, connection_string)[\"ORIGIN_FILE\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TAROUND_202008100101.CSV']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[fn.upper() for fn in ftp.nlst() \n",
    "              if filename_pattern in fn.upper() and fn.upper().endswith(extension) and fn.upper() not in current_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get missing files\n",
    "# file_names = [fn.upper() for fn in ftp.nlst()\n",
    "#               if filename_pattern in fn.upper() and fn.upper().endswith(extension) and fn.upper() not in current_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists on drive\n",
      "File TAROUND_202007130102.CSV uploaded successfully to the database\n",
      "File already exists on drive\n",
      "File TAROUND_202007260101.CSV uploaded successfully to the database\n",
      "File already exists on drive\n",
      "File TAROUND_202008100101.CSV uploaded successfully to the database\n",
      "File already exists on drive\n",
      "File TAROUND_202008160100.CSV uploaded successfully to the database\n",
      "File already exists on drive\n",
      "File TAROUND_202008170101.CSV uploaded successfully to the database\n",
      "File already exists on drive\n",
      "File TAROUND_202008220101.CSV uploaded successfully to the database\n",
      "File already exists on drive\n",
      "File TAROUND_202008240102.CSV uploaded successfully to the database\n",
      "Downloaded: TAROUND_202008270103.CSV\n",
      "File TAROUND_202008270103.CSV uploaded successfully to the database\n",
      "Downloaded: TAROUND_202008280102.CSV\n",
      "File TAROUND_202008280102.CSV uploaded successfully to the database\n"
     ]
    }
   ],
   "source": [
    "# Download missing files and save to db\n",
    "ftp = FTP(ftp_site, user=user, passwd=pw)\n",
    "ftp.cwd(ftp_path)\n",
    "file_names = [fn.upper() for fn in ftp.nlst() \n",
    "              if filename_pattern in fn.upper() and fn.upper().endswith(extension) and fn.upper() not in current_files]\n",
    "for file_name in file_names:\n",
    "    local_filename = os.path.join(path, file_name)\n",
    "    if not os.path.isfile(local_filename):\n",
    "        with open(local_filename, 'wb') as local_file:\n",
    "            ftp.retrbinary('RETR %s' % file_name, local_file.write)\n",
    "            print(\"Downloaded: %s\" % file_name);\n",
    "    else:\n",
    "        print(\"File already exists on drive\");\n",
    "    \n",
    "    insertTaroundFile(file_name, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Temp\\\\IIR\\\\'\n",
    "filename_pattern='TAROUND'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Delete all local files matching pattern\n",
    "# path = 'C:\\\\Temp\\\\IIR\\\\TAROUND'\n",
    "# filename_pattern='TAROUND'\n",
    "# file_names = [fn.upper() for fn in os.listdir(path)\n",
    "#               if filename_pattern in fn.upper() and fn.upper().endswith(extension)]\n",
    "# for name in file_names:\n",
    "#     filepath = os.path.join(path, name)\n",
    "#     try:\n",
    "#         os.remove(filepath)\n",
    "#     except:\n",
    "#         print(\"Error while deleting file : \", filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_filename = os.path.join(path, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: TAROUND_202008260102.CSV\n"
     ]
    }
   ],
   "source": [
    "with open(local_filename, 'wb') as f:\n",
    "        ftp.retrbinary('RETR %s' % name, f.write)\n",
    "        print(\"Downloaded: %s\" % name);    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
