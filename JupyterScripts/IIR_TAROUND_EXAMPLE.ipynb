{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.pool import StaticPool\n",
    "from decimal import Decimal\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#DB Details\n",
    "connection_type = \"mssql+pyodbc\"\n",
    "dataportal_db_user = \"dataportal\"\n",
    "dataportal_db_password = \"FatShamingMarc\"\n",
    "dataportal_db_ip = \"10.8.4.35\"\n",
    "db_port = \"1433\"\n",
    "db_prod_name = \"dataportal_prod\"\n",
    "db_driver = \"ODBC+DRIVER+17+for+SQL+Server\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_connection_string(connection_type, user, password, host, port, database_name, driver):\n",
    "    return (\n",
    "        \"{connection_type}://{user}:{password}@{host}:{port}/{database_name}\"\n",
    "        \"?driver={driver}\").format(\n",
    "        connection_type=connection_type,\n",
    "        user=user,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database_name=database_name,\n",
    "        driver=driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertTaroundFile(filename, path):\n",
    "    filepath = path + filename\n",
    "    #Define columns we're interested in from the file\n",
    "    columns=['DELV_DATE', 'OUTAGE_ID', 'UNIT_NAME', 'UNIT_ID', 'AREA_ID', 'AREA_NAME', 'OWNER_NAME', 'PLANT_ID', 'PLANT_NAME', \n",
    "             'U_STATUS', 'UTYPE_DESC', 'CHARGERATE', 'CAPACITY', 'TA_START', 'TA_END', 'PRECISION', 'OUTAGE_TYP', \n",
    "             'OUTAGE_STA', 'OUT_CAUSE', 'PERIOD', 'PAD_DIST', 'COMMENTS']\n",
    "    renamed_columns={\"DELV_DATE\": \"PUBLISH_DATE\", \"OUTAGE_ID\": \"EVENT_ID\", \"U_STATUS\":\"UNIT_STATUS\", \n",
    "                 \"CHARGERATE\":\"CAPACITY\", \"CAPACITY\":\"CAPACITY_OFFLINE\", \n",
    "                 \"TA_START\": \"EVENT_START\", \"TA_END\": \"EVENT_END\", \"PRECISION\": \"DATE_PRECISION\", \"OUTAGE_TYP\" : \"EVENT_TYPE\",\n",
    "                 \"OUTAGE_STA\": \"EVENT_STATUS\", \"OUT_CAUSE\": \"EVENT_CAUSE\", \"PERIOD\" : \"EVENT_DURATION\", \"PAD_DIST\": \"PAD_DISTRICT\"}\n",
    "    df = pd.read_csv(filepath, low_memory=False)[columns]\n",
    "    df = df.rename(columns=renamed_columns)\n",
    "    df[\"PUBLISH_DATE\"] = pd.to_datetime(df[\"PUBLISH_DATE\"], format='%Y%m%d')\n",
    "    df[\"EVENT_START\"] = pd.to_datetime(df[\"EVENT_START\"], format='%Y%m%d')\n",
    "    df[\"EVENT_END\"] = pd.to_datetime(df[\"EVENT_END\"], format='%Y%m%d')\n",
    "    df['ORIGIN_FILE'] = filename\n",
    "    df.to_sql('data_iir_offlineevents', connection_string, if_exists='append', index=False,\n",
    "        dtype={ 'PUBLISH_DATE' :   sa.DateTime(),\n",
    "                'EVENT_ID' :       sa.types.INTEGER(),\n",
    "                'UNIT_NAME' :     sa.types.VARCHAR(100),\n",
    "                'UNIT_ID' :       sa.types.INTEGER(),\n",
    "                'AREA_ID' :       sa.types.INTEGER(),\n",
    "                'AREA_NAME' :     sa.types.VARCHAR(100),\n",
    "                'OWNER_NAME' :     sa.types.VARCHAR(100),\n",
    "                'PLANT_ID' :       sa.types.INTEGER(),\n",
    "                'PLANT_NAME' :     sa.types.VARCHAR(100),\n",
    "                'UNIT_STATUS' : sa.types.VARCHAR(30),\n",
    "                'UTYPE_DESC' : sa.types.VARCHAR(100),\n",
    "                'CAPACITY' : sa.types.INTEGER(),\n",
    "                'CAPACITY_OFFLINE' : sa.types.INTEGER(),\n",
    "                'EVENT_START' :  sa.DateTime(),\n",
    "                'EVENT_END' :  sa.DateTime(),\n",
    "                'DATE_PRECISION' : sa.types.VARCHAR(16) ,\n",
    "                'EVENT_TYPE' : sa.types.VARCHAR(30),         \n",
    "                'EVENT_STATUS' : sa.types.VARCHAR(30),\n",
    "                'EVENT_CAUSE' : sa.types.VARCHAR(65),\n",
    "                'EVENT_DURATION' : sa.types.INTEGER(),\n",
    "                'PAD_DISTRICT' : sa.types.VARCHAR(65),\n",
    "                'COMMENTS' : sa.types.VARCHAR(2000) ,\n",
    "                'ORIGIN_FILE' : sa.types.VARCHAR(65)})\n",
    "    print(\"File %s uploaded successfully to the database\" % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftplib import FTP\n",
    "global ftp\n",
    "\n",
    "ftp_site='ftp.industrialinfo.com'\n",
    "user='andurand'\n",
    "pw='361984'\n",
    "path = 'C:\\\\Temp\\\\IIR\\\\TAROUND\\\\'\n",
    "ftp_path='/data/'\n",
    "extension='CSV'\n",
    "filename_pattern='TAROUND'\n",
    "\n",
    "#ftp = FTP(ftp_site, user=user, passwd=pw)\n",
    "#ftp.cwd(ftp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve list of current inserted files from the database\n",
    "sql_query = \"select distinct(ORIGIN_FILE) from data_iir_offlineevents where origin_file like 'TAROUND%'\"\n",
    "connection_string = create_connection_string(connection_type,dataportal_db_user,dataportal_db_password,dataportal_db_ip,db_port,db_prod_name, db_driver)\n",
    "current_files = [file.upper() for file in pd.read_sql(sql_query, connection_string)[\"ORIGIN_FILE\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: TAROUND_202009120101.CSV\n",
      "File TAROUND_202009120101.CSV uploaded successfully to the database\n",
      "Downloaded: TAROUND_202009130102.CSV\n",
      "File TAROUND_202009130102.CSV uploaded successfully to the database\n",
      "Downloaded: TAROUND_202009140101.CSV\n",
      "File TAROUND_202009140101.CSV uploaded successfully to the database\n"
     ]
    }
   ],
   "source": [
    "# Download missing files and save to db\n",
    "ftp = FTP(ftp_site, user=user, passwd=pw)\n",
    "ftp.cwd(ftp_path)\n",
    "file_names = [fn.upper() for fn in ftp.nlst() \n",
    "              if filename_pattern in fn.upper() and fn.upper().endswith(extension) and fn.upper() not in current_files]\n",
    "for file_name in file_names:\n",
    "    local_filename = os.path.join(path, file_name)\n",
    "    if not os.path.isfile(local_filename):\n",
    "        with open(local_filename, 'wb') as local_file:\n",
    "            ftp.retrbinary('RETR %s' % file_name, local_file.write)\n",
    "            print(\"Downloaded: %s\" % file_name);\n",
    "    else:\n",
    "        print(\"File already exists on drive\");\n",
    "    \n",
    "    insertTaroundFile(file_name, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
